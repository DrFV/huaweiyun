{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 配置环境\n",
    "实验平台：华为云ModelArts CPU 8核32GiB\n",
    "\n",
    "软件环境：python3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting joblib\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/b8/a6/d1a816b89aa1e9e96bcb298eb1ee1854f21662ebc6d55ffa3d7b3b50122b/joblib-0.15.1-py3-none-any.whl (298kB)\n",
      "\u001b[K    100% |████████████████████████████████| 307kB 33.2MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: joblib\n",
      "Successfully installed joblib-0.15.1\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/f3/76/4697ce203a3d42b2ead61127b35e5fcc26bba9a35c03b32a2bd342a4c869/tqdm-4.46.1-py2.py3-none-any.whl (63kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 64.9MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.46.1\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using MoXing-v1.15.1-99273b13\n",
      "INFO:root:Using OBS-Python-SDK-3.1.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "import moxing as mox\n",
    "from modelarts.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display设置\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ma-user/work/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_file_path = os.environ['HOME'] + '/work/'\n",
    "source_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:obs:Successfully download file etasll2020/port.csv from OBS to local /home/ma-user/work/data/port.csv\n",
      "INFO:obs:Successfully download file etasll2020/loadingOrderEvent.csv from OBS to local /home/ma-user/work/data/loadingOrderEvent.csv\n",
      "INFO:obs:Successfully download file etasll2020/A_testData0531.csv from OBS to local /home/ma-user/work/data/A_testData0531.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully download file etasll2020/port.csv from OBS to local /home/ma-user/work/data/port.csv\n",
      "Successfully download file etasll2020/loadingOrderEvent.csv from OBS to local /home/ma-user/work/data/loadingOrderEvent.csv\n",
      "Successfully download file etasll2020/A_testData0531.csv from OBS to local /home/ma-user/work/data/A_testData0531.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:obs:Successfully download file etasll2020/train0523.tar.gz from OBS to local /home/ma-user/work/data/train0523.tar.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully download file etasll2020/train0523.tar.gz from OBS to local /home/ma-user/work/data/train0523.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# 从OBS下载数据\n",
    "session = Session()\n",
    "session.download_data(bucket_path=\"etasll2020/port.csv\", path=source_file_path+\"data/port.csv\")\n",
    "session.download_data(bucket_path=\"etasll2020/loadingOrderEvent.csv\", path=source_file_path+\"data/loadingOrderEvent.csv\")\n",
    "session.download_data(bucket_path=\"etasll2020/A_testData0531.csv\", path=source_file_path+\"data/A_testData0531.csv\")\n",
    "session.download_data(bucket_path=\"etasll2020/train0523.tar.gz\", path=source_file_path+\"data/train0523.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "._train0523.csv\r\n",
      "train0523.csv\r\n"
     ]
    }
   ],
   "source": [
    "# 解压\n",
    "!tar -zxvf data/train0523.tar.gz -C data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NJOBS = 8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_data = pd.read_csv(\"data/port.csv\")\n",
    "order_data = pd.read_csv(\"data/loadingOrderEvent.csv\")\n",
    "test_data = pd.read_csv(\"data/A_testData0531.csv\")\n",
    "reader = pd.read_csv(\"data/train0523.csv\", header=None, chunksize=100000,iterator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 数据清洗\n",
    "\n",
    "训练集：\n",
    "1. 对每一行加入了下一个时刻的时间戳、经度、纬度特征\n",
    "2. 求出连续两个时刻的距离、滞留时间\n",
    "3. 对方向、速度的缺失值进行计算处理，求出当前方向与航线方向对偏角\n",
    "3. nextPort, nextETA, status的缺失值进行了bfill处理\n",
    "\n",
    "**注：由于训练集数据过大，所以采用分批处理和多线程处理的方式，以加速处理；测试集数据量较小，采用直接处理的方式**\n",
    "\n",
    "\n",
    "测试集：\n",
    "1. 对每一行加入了下一个时刻的时间戳、经度、纬度特征\n",
    "2. 求出连续两个时刻的距离、滞留时间\n",
    "3. 对方向、速度的缺失值进行计算处理，求出当前方向与航线方向对偏角\n",
    "\n",
    "其他数据：\n",
    "1. 将port_data中港口的名称转换成大写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 特征处理api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 经纬度处理函数\n",
    "# reference: https://github.com/yongruihuang/Courier-Competition-Round1/blob/9ceb73055d81eaa351b094c98fda43772fb0efeb/code/tools/pos_encoder.py\n",
    "\n",
    "def bearing_array(lat1, lng1, lat2, lng2):\n",
    "    \"\"\"\n",
    "    两个经纬度之间的方位\n",
    "    \"\"\"\n",
    "    AVG_EARTH_RADIUS = 6371 # in km\n",
    "    lng_delta_rad = np.radians(lng2 - lng1)\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    y = np.sin(lng_delta_rad) * np.cos(lat2)\n",
    "    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n",
    "    degree = np.degrees(np.arctan2(y, x))\n",
    "    return (degree + 360) % 360\n",
    "\n",
    "\n",
    "def haversine_array(lat1, lng1, lat2, lng2):\n",
    "    \"\"\"\n",
    "    Haversine距离\n",
    "    \"\"\"\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    AVG_EARTH_RADIUS = 6371 # in km\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n",
    "    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n",
    "    return h\n",
    "\n",
    "\n",
    "def dummy_manhattan_distance(lat1, lng1, lat2, lng2):\n",
    "    \"\"\"\n",
    "    曼哈顿距离\n",
    "    \"\"\"\n",
    "    a = haversine_array(lat1, lng1, lat1, lng2)\n",
    "    b = haversine_array(lat1, lng1, lat2, lng1)\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timeStamp(timeString, form=0):\n",
    "    if form == 0:\n",
    "        timeArray = time.strptime(timeString, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    if form == 1:\n",
    "        timeArray = time.strptime(timeString, '%Y/%m/%d %H:%M')\n",
    "    timeStamp = int(time.mktime(timeArray))\n",
    "    return timeStamp\n",
    "\n",
    "def dir_diff(temp_dir,total_dir):\n",
    "    \"\"\"\n",
    "    求每条sample的方向和总方向之间的差异\n",
    "    \"\"\"\n",
    "    raw_diff = np.abs(temp_dir-total_dir)\n",
    "    if raw_diff > 180:\n",
    "        diff = 360-raw_diff\n",
    "    else:\n",
    "        diff = raw_diff\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 训练集预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_grouped(subset): \n",
    "    # next position\n",
    "    subset['nextLongitude'] = subset['longitude'].shift(-1)\n",
    "    subset['nextLatitude'] = subset['latitude'].shift(-1)\n",
    "    \n",
    "    # direction\n",
    "    subset['direction'] = subset['direction'] / 100\n",
    "    empty = subset[subset['direction']==-0.01]\n",
    "    direct = bearing_array(empty['latitude'], empty['longitude'], empty['nextLatitude'], empty['nextLongitude'])\n",
    "    subset['direction'].replace(-0.01, direct, inplace=True)\n",
    "    \n",
    "    # timeStamp\n",
    "    subset['timestamp'] = subset['timestamp'].apply(get_timeStamp)\n",
    "    subset['nextTimestamp'] = subset['timestamp'].shift(-1)\n",
    "    \n",
    "    # distance\n",
    "    subset['distance'] = haversine_array(subset['latitude'], subset['longitude'], subset['nextLatitude'], subset['nextLongitude'])\n",
    "    # subset['distance'] = subset.fillna(0,inplace = True)\n",
    "\n",
    "    # speed\n",
    "    empty = subset[subset['speed']==0]\n",
    "    speed = empty['distance']/(empty['nextTimestamp']-empty['timestamp'])*3600 \n",
    "    subset['speed'].replace(0, speed, inplace=True)   \n",
    "\n",
    "    # vessel\n",
    "    subset['vesselNextport'] = subset['vesselNextport'].bfill()\n",
    "    subset['vesselNextportETA'] = subset['vesselNextportETA'].bfill()\n",
    "    subset['vesselStatus'] = subset['vesselStatus'].bfill()\n",
    "    subset['vesselStatus'] = subset['vesselStatus'].fillna(0)\n",
    "\n",
    "    # single anchor time\n",
    "    subset['anchor']=subset.apply(lambda x: 1 if x['vesselStatus'] == 'moored' or x['vesselStatus'] == 'at anchor'\n",
    "                        else 0, axis=1)\n",
    "    subset['singleAnchortime']=(subset['nextTimestamp']-subset['timestamp']) * subset['anchor']\n",
    "\n",
    "    # direction difference\n",
    "    final = subset.tail(1)\n",
    "    lastLongitude = final['longitude'].tolist()\n",
    "    lastLatitude = final['latitude'].tolist()\n",
    "    subset['total_dir'] = bearing_array(subset['latitude'], subset['longitude'], lastLatitude, lastLongitude)\n",
    "    subset['dirDiff'] = subset.apply(lambda x: dir_diff(x['direction'], x['total_dir']), axis=1)\n",
    "    \n",
    "    return subset\n",
    "\n",
    "\n",
    "def preprocess(data, show_progress=False):\n",
    "    \"\"\"\n",
    "    训练集预处理\n",
    "    \"\"\"\n",
    "    \n",
    "    data_grouped = data.groupby('loadingOrder') \n",
    "    if show_progress:\n",
    "        results = Parallel(n_jobs=NJOBS)(delayed(preprocess_grouped)(group) for name, group in tqdm(data_grouped))\n",
    "    else:\n",
    "        results = Parallel(n_jobs=NJOBS)(delayed(preprocess_grouped)(group) for name, group in data_grouped)\n",
    "    return pd.concat(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 测试集预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test(data):\n",
    "    \"\"\"\n",
    "    测试集预处理，所有处理都是inplace的\n",
    "    \"\"\"\n",
    "    data.sort_values(['loadingOrder','carrierName'], inplace=True)\n",
    "    \n",
    "    # next position\n",
    "    data.loc[:,'nextLongitude'] = data.groupby('loadingOrder')['longitude'].shift(-1)\n",
    "    data.loc[:,'nextLatitude'] = data.groupby('loadingOrder')['latitude'].shift(-1)\n",
    "    \n",
    "    # direction\n",
    "    data.loc[:, 'direction'] /= 100\n",
    "    empty = data[data['direction']==-0.01]\n",
    "    direct = bearing_array(empty['latitude'], empty['longitude'], empty['nextLatitude'], empty['nextLongitude'])\n",
    "    data.loc[:, 'direction'].replace(-0.01, direct, inplace=True)\n",
    "    \n",
    "    # time stamp\n",
    "    data.loc[:,'timestamp'] = data['timestamp'].apply(get_timeStamp)\n",
    "    data.loc[:,'nextTimestamp'] = data.groupby('loadingOrder')['timestamp'].shift(-1)\n",
    "    \n",
    "    # distance\n",
    "    data.loc[:,'distance'] = haversine_array(data['latitude'], data['longitude'], data['nextLatitude'], data['nextLongitude'])\n",
    "    \n",
    "    # speed\n",
    "    empty = data[data['speed']==0]\n",
    "    speed = empty['distance']/(empty['nextTimestamp']-empty['timestamp'])*3600 \n",
    "    data.loc[:, 'speed'].replace(0, speed, inplace=True)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 其他数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把港口名称名称大写\n",
    "port_data['TRANS_NODE_NAME'] = port_data['TRANS_NODE_NAME'].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 特征提取\n",
    "\n",
    "提取特征（共14+1个）\n",
    "\n",
    "起点终点的经纬度、滞留时间、航线距离、速度及偏角的最大/最小/均值/中位数、订单出现的次数（用于二次预处理）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 特征提取api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/country_abb.txt\") as f:\n",
    "#     country_list = f.readline()\n",
    "# country_list = country_list.split(',')\n",
    "    \n",
    "def unify_name(name, mode='test'):\n",
    "    \"\"\"\n",
    "    对nextPort命名不规范的名称进行处理\n",
    "    可能的不规范命名：XX XXX > XX XXX\n",
    "                      XX XXX\n",
    "                      XXX > XXX\n",
    "    \"\"\"\n",
    "    if mode == 'test':\n",
    "        name = name.split('-')[-1].strip()\n",
    "        return name\n",
    "    \n",
    "# 训练集中数据量太大，处理时间太长，暂不考虑处理\n",
    "#     name = name.upper()\n",
    "#     raw_name = name\n",
    "#     try:\n",
    "#         if '>' in name:\n",
    "#             if len(name.split('>')[-1].strip()) == 1: \n",
    "#                 name = name.split('>')[0].strip()\n",
    "#             else:\n",
    "#                 name = name.split('>')[-1].strip()\n",
    "#         if '-' in name:\n",
    "#             if len(name.split('-')[-1].strip()) == 1:\n",
    "#                 name = name.split('-')[0].strip()\n",
    "#             else:\n",
    "#                 name = name.split('-')[-1].strip()\n",
    "#         if '<' in name:\n",
    "#             name = name.split('<')[0].strip()\n",
    "#         names = name.split()\n",
    "#         if names[0] in country_list:\n",
    "#             name = names[-1]\n",
    "#         else:\n",
    "#             name = names[0]\n",
    "#         return name\n",
    "#     except IndexError:\n",
    "#         print(raw_name)\n",
    "#         print(\"***********************************\")\n",
    "#         return raw_name\n",
    "\n",
    "    \n",
    "def find_possible_ports(name):\n",
    "    \"\"\"\n",
    "    对name在port_data中找可能对应的名字，返回列表\n",
    "    \"\"\"\n",
    "    # 如果恰有相同名称的直接返回index （可能仍然有多个index: 重名）\n",
    "    if len(port_data[port_data['TRANS_NODE_NAME']==name]) != 0: \n",
    "        return port_data[port_data['TRANS_NODE_NAME']==name].index.to_list()\n",
    "    # 否则查找名称是否有包含name的\n",
    "    _possible_ports = []\n",
    "    for p in port_data['TRANS_NODE_NAME'].unique():            \n",
    "        if name in p:\n",
    "            _possible_ports.extend(port_data[port_data['TRANS_NODE_NAME']==p].index.to_list())\n",
    "    return _possible_ports\n",
    "    \n",
    "    \n",
    "def get_test_destination(test_data,loadingOrder):\n",
    "    \"\"\"\n",
    "    找到测试集中的出发点的经纬度\n",
    "    \"\"\"\n",
    "    _test_data = test_data[test_data['loadingOrder']==loadingOrder]\n",
    "    path = _test_data['TRANSPORT_TRACE'].unique()[0]\n",
    "    des_name = unify_name(path, mode='test')\n",
    "    possible_ports = find_possible_ports(des_name)\n",
    "    if len(possible_ports)==0:\n",
    "        return -1\n",
    "    des_lon = port_data.loc[possible_ports,'LONGITUDE'].values[0]\n",
    "    des_lat = port_data.loc[possible_ports,'LATITUDE'].values[0]\n",
    "    return des_lon, des_lat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 训练集特征提取函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_grouped(subset):\n",
    "    ans = subset.head(1)\n",
    "    \n",
    "    # anchor time\n",
    "    anchorTime = subset['singleAnchortime'].sum()\n",
    "    ans['anchorTime'] = anchorTime\n",
    "    \n",
    "    # speed\n",
    "    speed = subset['speed'].agg({'maxSpeed': 'max', 'minSpeed': 'min','meanSpeed':'mean','medianSpeed':'median'})\n",
    "    ans['maxSpeed'],ans['minSpeed'],ans['meanSpeed'],ans['medianSpeed'] = speed\n",
    "    \n",
    "    # direction\n",
    "    diff = subset['dirDiff'].agg({'maxDirdiff': 'max', 'minDirdiff': 'min','meanDirdiff':'mean','medianDirdiff':'median'})\n",
    "    ans['maxDirdiff'],ans['minDirdiff'],ans['meanDirdiff'],ans['medianDirdiff'] = diff\n",
    "    \n",
    "    # distance\n",
    "    first = subset.head(1)\n",
    "    final = subset.tail(1)\n",
    "    lastLongitude = final['longitude'].to_list()[0]\n",
    "    lastLatitude = final['latitude'].to_list()[0]\n",
    "    ans['lastLatitude'] = lastLatitude\n",
    "    ans['lastLongitude'] = lastLongitude\n",
    "    ans['manDis'] = haversine_array(ans['latitude'].to_list()[0], ans['longitude'].to_list()[0], lastLatitude, lastLongitude)\n",
    "    \n",
    "    # time interval\n",
    "    ans['timeInterval'] = final['timestamp'].to_list()[0] - first['timestamp'].to_list()[0]\n",
    "    \n",
    "    ans['number'] = len(subset)\n",
    "    \n",
    "    feature = ['loadingOrder', 'longitude', 'latitude', 'lastLongitude', 'lastLatitude',\n",
    "   'anchorTime', 'manDis', 'maxSpeed', 'minSpeed', 'meanSpeed', 'medianSpeed',\n",
    "   'maxDirdiff', 'minDirdiff', 'meanDirdiff', 'medianDirdiff', 'timeInterval', 'number'] \n",
    "    \n",
    "    return ans[feature]\n",
    "\n",
    "\n",
    "def add_feature(data, show_progress=False):\n",
    "    data_grouped = data.groupby('loadingOrder')\n",
    "    if show_progress:\n",
    "        feature = Parallel(n_jobs=NJOBS)(delayed(add_feature_grouped)(group) for name, group in tqdm(data_grouped))\n",
    "    else:\n",
    "        feature = Parallel(n_jobs=NJOBS)(delayed(add_feature_grouped)(group) for name, group in data_grouped)\n",
    "    return pd.concat(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 测试集特征提取函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_test(data):\n",
    "    \"\"\"\n",
    "    将相同订单号的数据合并并添加特征\n",
    "    @param:\n",
    "    data：测试数据预处理过的训练数据\n",
    "    mode:训练集和测试集有不同的处理方式\n",
    "    \"\"\"\n",
    "    df = data\n",
    "    \n",
    "    df['anchor']=df.apply(lambda x: 1 if x['speed'] == 0 else 0, axis=1)\n",
    "    df['singleAnchortime']=(df['nextTimestamp']-df['timestamp']) * df['anchor']\n",
    "    anchorTime = df.groupby('loadingOrder')['singleAnchortime'].sum().tolist()\n",
    "\n",
    "    ans = df.groupby('loadingOrder').head(1)\n",
    "    total_dis = df.groupby('loadingOrder')['distance'].sum().tolist()\n",
    "    ans['totalDis']=total_dis\n",
    "    group = df.groupby('loadingOrder')['speed'].agg({'maxSpeed': 'max', 'minSpeed': 'min','meanSpeed':'mean','medianSpeed':'median'}).reset_index()\n",
    "    ans=ans.merge(group,on='loadingOrder',how='left')\n",
    "    ans['anchorTime']=anchorTime\n",
    "    ans['lastLongitude']=ans['loadingOrder'].apply(lambda x: get_test_destination(test_data,x)[0])\n",
    "    ans['lastLatitude']=ans['loadingOrder'].apply(lambda x: get_test_destination(test_data,x)[1])\n",
    "    ans['total_dir']=bearing_array(ans['latitude'],ans['longitude'],ans['lastLatitude'],ans['lastLongitude'])\n",
    "    temp=ans[['loadingOrder','total_dir']]\n",
    "    df = df.merge(temp,on = 'loadingOrder',how='left')\n",
    "    df['dirDiff']= df.apply(lambda x: dir_diff(x['direction'],x['total_dir']),axis =1)\n",
    "    diff = df.groupby('loadingOrder')['dirDiff'].agg({'maxDirdiff': 'max', 'minDirdiff': 'min','meanDirdiff':'mean','medianDirdiff':'median'}).reset_index()\n",
    "\n",
    "    ans=ans.merge(diff,on='loadingOrder',how='left')\n",
    "    ans['manDis']=haversine_array(ans['latitude'],ans['longitude'],ans['lastLatitude'],ans['lastLongitude'])\n",
    "    feature = ['loadingOrder', 'longitude', 'latitude', 'lastLongitude', 'lastLatitude',\n",
    "       'anchorTime', 'manDis', 'maxSpeed', 'minSpeed', 'meanSpeed', 'medianSpeed',\n",
    "       'maxDirdiff', 'minDirdiff', 'meanDirdiff', 'medianDirdiff']\n",
    "    ans = ans[feature]\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 运行\n",
    "\n",
    "调用以上的函数进行处理，并将处理好数据的保存到桶里"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_feature.csv saved!\n"
     ]
    }
   ],
   "source": [
    "preprocess_test(test_data)\n",
    "test_feature = add_feature_test(test_data)\n",
    "test_feature.to_csv(\"data/test_feature.csv\", index=False)\n",
    "mox.file.copy_parallel(\"data/test_feature.csv\", \"s3://etasll2020/saved/test_feature.csv\")\n",
    "print(\"test_feature.csv saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature200_new/feature1.csv saved!\n"
     ]
    }
   ],
   "source": [
    "loop = True\n",
    "i = 0\n",
    "while loop:\n",
    "    try:\n",
    "        i+=1\n",
    "        chunk = reader.get_chunk()\n",
    "        chunk.columns = ['loadingOrder','carrierName','timestamp','longitude',\n",
    "                  'latitude','vesselMMSI','speed','direction','vesselNextport',\n",
    "                  'vesselNextportETA','vesselStatus','vesselDatasource','TRANSPORT_TRACE']\n",
    "        train_data = preprocess(chunk)        \n",
    "        features = add_feature(train_data)\n",
    "        filepath = \"feature200_new/feature\"+str(i)+\".csv\"\n",
    "        features.to_csv(filepath)\n",
    "        mox.file.copy_parallel(filepath, 's3://etasll2020/saved/'+filepath) \n",
    "        print(\"feature200_new/feature\"+str(i)+\".csv saved!\")\n",
    "        break\n",
    "    except StopIteration:\n",
    "        loop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
